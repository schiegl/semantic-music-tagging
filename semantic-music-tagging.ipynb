{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.8.1-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python38164bitmusicanalysiscondabe7ff345cf3449e1b0127679bdb8f85e",
   "display_name": "Python 3.8.1 64-bit ('music-analysis': conda)"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Music Tagging\n",
    "\n",
    "The necessary dependencies to run this notebook is described in `environment.yml`. It can also be created automatically with Anaconda: `conda env create -f environment.yml`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from collections import OrderedDict\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchaudio\n",
    "from torchaudio.transforms import MelSpectrogram, AmplitudeToDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "**Source:** http://mirg.city.ac.uk/codeapps/the-magnatagatune-dataset\n",
    "\n",
    "### Structure\n",
    "\n",
    "- Clip info: `data/clip_info_final.csv`\n",
    "- Annotation info: `data/annotations_final.csv`\n",
    "- MP3 files: `data/audio`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>clip_id</th>\n      <th>track_number</th>\n      <th>title</th>\n      <th>artist</th>\n      <th>album</th>\n      <th>url</th>\n      <th>segmentStart</th>\n      <th>segmentEnd</th>\n      <th>original_url</th>\n      <th>mp3_path</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2</td>\n      <td>1</td>\n      <td>BWV54 - I Aria</td>\n      <td>American Bach Soloists</td>\n      <td>J.S. Bach Solo Cantatas</td>\n      <td>http://www.magnatune.com/artists/albums/abs-so...</td>\n      <td>30</td>\n      <td>59</td>\n      <td>http://he3.magnatune.com/all/01--BWV54%20-%20I...</td>\n      <td>f/american_bach_soloists-j_s__bach_solo_cantat...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>6</td>\n      <td>1</td>\n      <td>BWV54 - I Aria</td>\n      <td>American Bach Soloists</td>\n      <td>J.S. Bach Solo Cantatas</td>\n      <td>http://www.magnatune.com/artists/albums/abs-so...</td>\n      <td>146</td>\n      <td>175</td>\n      <td>http://he3.magnatune.com/all/01--BWV54%20-%20I...</td>\n      <td>f/american_bach_soloists-j_s__bach_solo_cantat...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>10</td>\n      <td>1</td>\n      <td>BWV54 - I Aria</td>\n      <td>American Bach Soloists</td>\n      <td>J.S. Bach Solo Cantatas</td>\n      <td>http://www.magnatune.com/artists/albums/abs-so...</td>\n      <td>262</td>\n      <td>291</td>\n      <td>http://he3.magnatune.com/all/01--BWV54%20-%20I...</td>\n      <td>f/american_bach_soloists-j_s__bach_solo_cantat...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>11</td>\n      <td>1</td>\n      <td>BWV54 - I Aria</td>\n      <td>American Bach Soloists</td>\n      <td>J.S. Bach Solo Cantatas</td>\n      <td>http://www.magnatune.com/artists/albums/abs-so...</td>\n      <td>291</td>\n      <td>320</td>\n      <td>http://he3.magnatune.com/all/01--BWV54%20-%20I...</td>\n      <td>f/american_bach_soloists-j_s__bach_solo_cantat...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>12</td>\n      <td>1</td>\n      <td>BWV54 - I Aria</td>\n      <td>American Bach Soloists</td>\n      <td>J.S. Bach Solo Cantatas</td>\n      <td>http://www.magnatune.com/artists/albums/abs-so...</td>\n      <td>320</td>\n      <td>349</td>\n      <td>http://he3.magnatune.com/all/01--BWV54%20-%20I...</td>\n      <td>f/american_bach_soloists-j_s__bach_solo_cantat...</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "   clip_id  track_number           title                  artist  \\\n0        2             1  BWV54 - I Aria  American Bach Soloists   \n1        6             1  BWV54 - I Aria  American Bach Soloists   \n2       10             1  BWV54 - I Aria  American Bach Soloists   \n3       11             1  BWV54 - I Aria  American Bach Soloists   \n4       12             1  BWV54 - I Aria  American Bach Soloists   \n\n                     album                                                url  \\\n0  J.S. Bach Solo Cantatas  http://www.magnatune.com/artists/albums/abs-so...   \n1  J.S. Bach Solo Cantatas  http://www.magnatune.com/artists/albums/abs-so...   \n2  J.S. Bach Solo Cantatas  http://www.magnatune.com/artists/albums/abs-so...   \n3  J.S. Bach Solo Cantatas  http://www.magnatune.com/artists/albums/abs-so...   \n4  J.S. Bach Solo Cantatas  http://www.magnatune.com/artists/albums/abs-so...   \n\n   segmentStart  segmentEnd  \\\n0            30          59   \n1           146         175   \n2           262         291   \n3           291         320   \n4           320         349   \n\n                                        original_url  \\\n0  http://he3.magnatune.com/all/01--BWV54%20-%20I...   \n1  http://he3.magnatune.com/all/01--BWV54%20-%20I...   \n2  http://he3.magnatune.com/all/01--BWV54%20-%20I...   \n3  http://he3.magnatune.com/all/01--BWV54%20-%20I...   \n4  http://he3.magnatune.com/all/01--BWV54%20-%20I...   \n\n                                            mp3_path  \n0  f/american_bach_soloists-j_s__bach_solo_cantat...  \n1  f/american_bach_soloists-j_s__bach_solo_cantat...  \n2  f/american_bach_soloists-j_s__bach_solo_cantat...  \n3  f/american_bach_soloists-j_s__bach_solo_cantat...  \n4  f/american_bach_soloists-j_s__bach_solo_cantat...  "
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clips = pd.read_csv('./data/clip_info_final.csv', delimiter='\\t')\n",
    "clips.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>clip_id</th>\n      <th>no voice</th>\n      <th>singer</th>\n      <th>duet</th>\n      <th>plucking</th>\n      <th>hard rock</th>\n      <th>world</th>\n      <th>bongos</th>\n      <th>harpsichord</th>\n      <th>female singing</th>\n      <th>...</th>\n      <th>rap</th>\n      <th>metal</th>\n      <th>hip hop</th>\n      <th>quick</th>\n      <th>water</th>\n      <th>baroque</th>\n      <th>women</th>\n      <th>fiddle</th>\n      <th>english</th>\n      <th>mp3_path</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>f/american_bach_soloists-j_s__bach_solo_cantat...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>6</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>f/american_bach_soloists-j_s__bach_solo_cantat...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>10</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>f/american_bach_soloists-j_s__bach_solo_cantat...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>11</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>f/american_bach_soloists-j_s__bach_solo_cantat...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>12</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>f/american_bach_soloists-j_s__bach_solo_cantat...</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 190 columns</p>\n</div>",
      "text/plain": "   clip_id  no voice  singer  duet  plucking  hard rock  world  bongos  \\\n0        2         0       0     0         0          0      0       0   \n1        6         0       0     0         0          0      0       0   \n2       10         0       0     0         0          0      0       0   \n3       11         0       0     0         0          0      0       0   \n4       12         0       0     0         0          0      0       0   \n\n   harpsichord  female singing  ...  rap  metal  hip hop  quick  water  \\\n0            0               0  ...    0      0        0      0      0   \n1            0               0  ...    0      0        0      0      0   \n2            0               0  ...    0      0        0      0      0   \n3            0               0  ...    0      0        0      0      0   \n4            0               0  ...    0      0        0      0      0   \n\n   baroque  women  fiddle  english  \\\n0        0      0       0        0   \n1        1      0       0        0   \n2        0      0       0        0   \n3        0      0       0        0   \n4        0      0       0        0   \n\n                                            mp3_path  \n0  f/american_bach_soloists-j_s__bach_solo_cantat...  \n1  f/american_bach_soloists-j_s__bach_solo_cantat...  \n2  f/american_bach_soloists-j_s__bach_solo_cantat...  \n3  f/american_bach_soloists-j_s__bach_solo_cantat...  \n4  f/american_bach_soloists-j_s__bach_solo_cantat...  \n\n[5 rows x 190 columns]"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotations = pd.read_csv('./data/annotations_final.csv', delimiter='\\t')\n",
    "annotations.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter dataset\n",
    "Remove damaged/empty files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# found with \"find <dir> -type f -empty\"\n",
    "empty_files = np.array([\n",
    "    '8/jacob_heringman-josquin_des_prez_lute_settings-19-gintzler__pater_noster-204-233.mp3',\n",
    "    '9/american_baroque-dances_and_suites_of_rameau_and_couperin-25-le_petit_rien_xiveme_ordre_couperin-88-117.mp3',\n",
    "    '6/norine_braun-now_and_zen-08-gently-117-146.mp3'\n",
    "])\n",
    "\n",
    "annotations = annotations[~annotations['mp3_path'].isin(empty_files)]\n",
    "clips = clips[~clips['mp3_path'].isin(empty_files)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter clips that don't belong to 50 most frequent tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array(['guitar', 'classical', 'slow', 'techno', 'strings', 'drums',\n       'electronic', 'rock', 'fast', 'piano', 'ambient', 'beat', 'violin',\n       'vocal', 'synth', 'female', 'indian', 'opera', 'male', 'singing',\n       'vocals', 'no vocals', 'harpsichord', 'loud', 'quiet', 'flute',\n       'woman', 'male vocal', 'no vocal', 'pop', 'soft', 'sitar', 'solo',\n       'man', 'classic', 'choir', 'voice', 'new age', 'dance',\n       'male voice', 'female vocal', 'beats', 'harp', 'cello', 'no voice',\n       'weird', 'country', 'metal', 'female voice', 'choral'],\n      dtype=object)"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top50_tags = annotations.iloc[:,1:-1].sum(axis=0).sort_values(ascending=False)[:50].index.to_numpy()\n",
    "annotations = annotations[annotations[top50_tags].sum(axis=1) > 0]\n",
    "tags_to_remove = annotations.columns[1:-1].difference(top50_tags)\n",
    "annotations = annotations.drop(tags_to_remove, axis=1)\n",
    "clips = clips[clips['clip_id'].isin(annotations['clip_id'])]\n",
    "top50_tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split dataset\n",
    "Split dataset randomly while making sure that clips from the same track don't end up in different splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Train clips: 15902\nVal clips: 3393\nTest clips: 1813\n"
    }
   ],
   "source": [
    "# split tracks\n",
    "track_nums = clips['track_number'].unique()\n",
    "np.random.shuffle(track_nums)\n",
    "train_split, val_split, test_split = np.split(track_nums, [int(len(track_nums) * 0.8), int(len(track_nums) * 0.9)])\n",
    "\n",
    "# assign all clips from tracks to their corresponding split\n",
    "def clip_files_from_tracks(track_nums):\n",
    "    relevant_clips = clips[clips['track_number'].isin(track_nums)]\n",
    "    df = relevant_clips.merge(annotations.drop('mp3_path', axis=1))\n",
    "    labels = df[top50_tags].to_numpy()\n",
    "    files = 'data/audio/' + df['mp3_path'].to_numpy()\n",
    "    return files, labels\n",
    "\n",
    "train_clips, train_labels = clip_files_from_tracks(train_split)\n",
    "val_clips, val_labels = clip_files_from_tracks(val_split)\n",
    "test_clips, test_labels = clip_files_from_tracks(test_split)\n",
    "\n",
    "print('Train clips:', len(train_clips))\n",
    "print('Val clips:', len(val_clips))\n",
    "print('Test clips:', len(test_clips))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset\n",
    "\n",
    "Try to adhere to the parameters used in the ference implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_MELS = 96\n",
    "N_FFT = 512\n",
    "BATCH_SIZE = 24\n",
    "\n",
    "class MagnaTagATuneDataset(Dataset):\n",
    "    def __init__(self, files, labels, sample_rate=12000):\n",
    "        self.files = files\n",
    "        self.labels = torch.from_numpy(labels.astype(np.float32)) # pytorch expects 32-bit floats\n",
    "        self.sample_rate = sample_rate\n",
    "        self.transform = nn.Sequential(\n",
    "            MelSpectrogram(\n",
    "                sample_rate=sample_rate,\n",
    "                n_mels=N_MELS,\n",
    "                n_fft=N_FFT,\n",
    "                hop_length=256\n",
    "            ),\n",
    "            AmplitudeToDB()\n",
    "        )\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        file = self.files[index]\n",
    "        waveform, _ = torchaudio.load(file)\n",
    "        mel_spec = self.transform(waveform)\n",
    "        return mel_spec, self.labels[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Batch shape = torch.Size([24, 1, 96, 1821])\nLabel shape =  torch.Size([24, 50])\n"
    }
   ],
   "source": [
    "loader = {\n",
    "    mode: DataLoader(MagnaTagATuneDataset(clips, labels), shuffle=True, batch_size=BATCH_SIZE, num_workers=4, pin_memory=True)\n",
    "    for mode, clips, labels in [('train', train_clips, train_labels), ('val', val_clips, val_labels), ('test', test_clips, test_labels)]\n",
    "}\n",
    "\n",
    "print('Batch shape =', next(iter(loader['test']))[0].shape)\n",
    "print('Label shape = ', next(iter(loader['test']))[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "It would be nice to batch normalize along frequency dimension and not both for the input normalization, but this doesn't seem to be implemented in PyTorch: https://github.com/pytorch/pytorch/issues/21856"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(OrderedDict([\n",
    "    ('input',\n",
    "        nn.BatchNorm2d(1)\n",
    "        \n",
    "    ),\n",
    "    ('block1',\n",
    "        nn.Sequential(\n",
    "            nn.Conv2d(1, 64, 3),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ELU(),\n",
    "            nn.MaxPool2d((2,4))\n",
    "        )\n",
    "    ),\n",
    "    ('block2',\n",
    "        nn.Sequential(\n",
    "            nn.Conv2d(64, 128, 3),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ELU(),\n",
    "            nn.MaxPool2d((2,4))\n",
    "        )\n",
    "    ),\n",
    "    ('block3',\n",
    "        nn.Sequential(\n",
    "            nn.Conv2d(128, 128, 3),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ELU(),\n",
    "            nn.MaxPool2d((2,4))\n",
    "        )\n",
    "    ),\n",
    "    ('block4',\n",
    "        nn.Sequential(\n",
    "            nn.Conv2d(128, 64, 3),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ELU(),\n",
    "            nn.MaxPool2d((3,5))\n",
    "        )\n",
    "    ),\n",
    "    ('output',\n",
    "        nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(640, 50),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    )\n",
    "]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Move model to GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "device(type='cuda', index=0)"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train model for a maximum of 10 epoch with early stopping when the AUC score on the validation dataset got worse 2 epochs in a row. In this case use the model weights that scored the highest AUC on the validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, num_epochs=10):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_auc = 0.0\n",
    "    dataset_sizes = {'train': len(loader['train']), 'val': len(loader['val']) }\n",
    "    no_improv = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'\\nEpoch {epoch + 1}/{num_epochs}')\n",
    "        print('-' * 10)\n",
    "\n",
    "        # each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_auc = 0\n",
    "\n",
    "            for inputs, labels in loader[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    output = model(inputs)\n",
    "                    preds = (output > 0.5).float()\n",
    "                    loss = criterion(output, labels)\n",
    "\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                running_loss += loss.item()\n",
    "                running_auc += roc_auc_score(labels.data.cpu(), preds.cpu(), 'samples')\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_auc = running_auc / dataset_sizes[phase]\n",
    "\n",
    "            print(f'{phase} Loss: {epoch_loss:.4f} AUC: {epoch_auc:.4f}')\n",
    "\n",
    "            if phase == 'val':\n",
    "                if epoch_auc > best_auc:\n",
    "                    best_auc = epoch_auc\n",
    "                    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                    no_improv = 0\n",
    "                else:\n",
    "                    no_improv += 1\n",
    "\n",
    "        # early stopping\n",
    "        if no_improv > 2:\n",
    "            break\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print(f'\\nTraining complete in {time_elapsed // 60:.0f}m {time_elapsed % 60 :.0f}s')\n",
    "    print(f'Best val AUC: {best_auc:.4f}')\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "\nEpoch 1/10\n----------\ntrain Loss: 0.1850 AUC: 0.6006\nval Loss: 0.2075 AUC: 0.6049\n\nEpoch 2/10\n----------\ntrain Loss: 0.1601 AUC: 0.6675\nval Loss: 0.1745 AUC: 0.6555\n\nEpoch 3/10\n----------\ntrain Loss: 0.1525 AUC: 0.6871\nval Loss: 0.1749 AUC: 0.7015\n\nEpoch 4/10\n----------\ntrain Loss: 0.1480 AUC: 0.6974\nval Loss: 0.1682 AUC: 0.6943\n\nEpoch 5/10\n----------\ntrain Loss: 0.1442 AUC: 0.7056\nval Loss: 0.1647 AUC: 0.6784\n\nEpoch 6/10\n----------\ntrain Loss: 0.1405 AUC: 0.7121\nval Loss: 0.1750 AUC: 0.7142\n\nEpoch 7/10\n----------\ntrain Loss: 0.1368 AUC: 0.7189\nval Loss: 0.1664 AUC: 0.6835\n\nEpoch 8/10\n----------\ntrain Loss: 0.1331 AUC: 0.7262\nval Loss: 0.1684 AUC: 0.6984\n\nEpoch 9/10\n----------\ntrain Loss: 0.1291 AUC: 0.7335\nval Loss: 0.1593 AUC: 0.7013\n\nTraining complete in 29m 47s\nBest val AUC: 0.7142\n"
    }
   ],
   "source": [
    "trained_model = train_model(\n",
    "    model,\n",
    "    nn.BCELoss(),\n",
    "    optim.Adam(model.parameters())\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model):\n",
    "    n_test_batches = len(loader['test'])\n",
    "    total_auc = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in loader['test']:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            output = model(inputs)\n",
    "            preds = (output > 0.1).float()\n",
    "            total_auc += roc_auc_score(labels.data.cpu(), preds.cpu(), 'samples')\n",
    "\n",
    "    print(f'AUC: {total_auc / n_test_batches:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "AUC: 0.819\n"
    }
   ],
   "source": [
    "eval_model(trained_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I was not quite able to reproduce the high performance of the FCN4 model. This despite the preprocessing and network architecture being very similar with the only difference being that in the [reference implementation](https://github.com/keunwoochoi/music-auto_tagging-keras) (FCN5) they used 128 feature output dimensions for the 4th convolutional block. Instead of using 128 dimensions I opted to use fewer dimensions (64) to conform to the last layer which is also smaller with 64 dimensions. Another difference was that PyTorch did not support batch normalization along axes which Keras does.\n",
    "\n",
    "Maybe the few architectural differences can account for the 7% difference (81.9% vs 89.4%) in performance. Or maybe a different dataset split methodology was used which was not explained in the paper. I opted for the popular 80%/10%/10% split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}